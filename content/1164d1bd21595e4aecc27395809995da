<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Nonparametric Bayes Tutorial</TITLE>
<META NAME="description" CONTENT="Nonparametric Bayes Tutorial">
<META NAME="keywords" CONTENT="npb-tutorial">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="npb-tutorial.css">

</HEAD>

<BODY >

<P>

<P>
 

<P>
<DIV ALIGN="CENTER">

    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:18pt;">
<SPAN  CLASS="textbf">Foundations of Nonparametric Bayesian Methods</SPAN>
</font>
<BR>  
  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:12pt;">
  
    <A NAME="tex2html1"
  HREF="http://mlg.eng.cam.ac.uk/porbanz">Peter Orbanz</A>
</font>

</DIV>

<P>
This 3-part tutorial addresses a machine learning audience, not assumed
to be familiar with measure theory or the theory of stochastic processes.
The course is intended to provide (1) an overview of what
nonparametric Bayesian models exist
beyond those already used in machine learning, and (2) a basic
understanding of the mathematical construction of ''process'' models,
both existing ones and new models on a variety of possible domains.

<P>
<B>Part I: Basics</B> [<A NAME="tex2html44"
  HREF="./npb_part1.pdf">Slides</A>]

<P>
At least half of the first part will probably be spent reviewing
concepts from measure-theoretic probability (and
motivating why we need them for Bayesian nonparametrics). 
We will then define Bayesian 
estimation in these terms, introduce the basic construction tools
for stochastic processes, and see how the Gaussian and
Dirichlet processes are constructed from finite-dimensional
Gaussian and Dirichlet distributions. 

<P>
<B>Part II: Models on the simplex</B> [<A NAME="tex2html45"
  HREF="./npb_part2.pdf">Slides</A>]

<P>
Most of the existing Bayesian nonparametric literature, especially in
statistics, focusses on models on the simplex, i.e. probabilities on
probabilities. This second part will discuss different classes of
existing models and their properties, including Dirichlet, tailfree
and Levy processes.

<P>
<B>Part III: Construction of new models</B> [<A NAME="tex2html46"
  HREF="./npb_part3.pdf">Slides</A>]

<P>
Recent works in machine learning consider the construction of models
on other domains than the simplex, that is, models which nonparametrically
generate objects other than probability distributions (such as the
binary matrices generated by the Indian Buffet Process). We discuss
how nonparametric Bayesian models can be constructed on arbitrary 
domains, and what limitations we will have to expect for such constructions.

<P>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">References</SPAN>
</font>

<P>
I have added MathSciNet links for most references. Following the ``Article''
link on the MathSciNet page will take you directly to the article
in question.

<P>

<OL>
<LI><A HREF="#sec:general">General Reading and Surveys</A>
</LI>
<LI><A HREF="#sec:npbmodels">Nonparametric Bayesian Models</A>
  
<UL>
<LI><A HREF="#sec:DP">Dirichlet processes</A>
</LI>
<LI><A HREF="#sec:PT">P&#243;lya trees</A>
</LI>
<LI><A HREF="#sec:NTTR">Neutral to the right and L&#233;vy processes</A>
</LI>
<LI><A HREF="#sec:PY">The Pitman-Yor process</A>
</LI>
<LI><A HREF="#sec:consistency">Consistency and posterior convergence</A>
  
</LI>
</UL>
</LI>
<LI><A HREF="#sec:textbooks">Probability Textbooks</A>
</LI>
<LI><A HREF="#sec:math">Mathematical topics</A>
  
<UL>
<LI><A HREF="#sec:exch">Exchangeability</A>
</LI>
<LI><A HREF="#sec:sufficiency">Sufficiency</A>
</LI>
<LI><A HREF="#sec:pitman:koopman">Pitman-Koopman theory</A>
  
</LI>
</UL>
</LI>
</OL>

<P>
<DIV ALIGN="CENTER">

    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">General Reading and Surveys</SPAN>
</font>
        <A NAME="sec:general"></A>
</DIV>

<P>
A machine learning introduction to nonparametric Bayes 
that does take into account some
theory, is well written and beautifully illustrated, is
given by Erik Sudderth in his thesis.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Sudderth:2006">
        E.&nbsp;B. Sudderth.
        <BR><SPAN  CLASS="textit">Graphical Models for Visual Object Recognition and Tracking</SPAN>.
        <BR>
PhD thesis, 2006.
        [<A NAME="tex2html2"
  HREF="http://www.cs.brown.edu/~sudderth/papers/sudderthPhD.pdf">PDF</A>]

<P></A>
<DD> </SMALL>

<P>
A very good reference on abstract Bayesian methods, exchangeability,
sufficiency, and parametric
models (including infinite-dimensional Bayesian models) are the first
two chapters of Schervish's <EM>Theory of Statistics</EM>. This is a
serious math book, in which all results are presented with proofs. It also 
contains an excellent chapter on decision theory, and an appendix
providing a concise reference summary of the probability theory involved.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Schervish:1995">
        M.&nbsp;J. Schervish.
        <BR><SPAN  CLASS="textit">Theory of Statistics</SPAN>.
        <BR>
Springer, 1995.
        [<A NAME="tex2html3"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1354146">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
An overview article on different models for random measures
(ie roughly on the material in the second talk) is the following:

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Walker:Damien:Laud:Smith:1999">
       S.&nbsp;G. Walker, P.&nbsp;Damien, P.&nbsp;W. Laud, and A.&nbsp;F.&nbsp;M. Smith.
       <BR>
Bayesian nonparametric inference for random distributions and related
                 functions.
       <BR><SPAN  CLASS="textit">Journal of the Royal Statistical Society B</SPAN>, 61(3):485-527, 1999.
       [<A NAME="tex2html4"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1707858">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
The following monograph is a useful but fairly technical overview,
with a strong focus on issues of consistency.
For anybody with a machine learning background interested in
consistency, I believe the Ghosal/van der
Vaart papers below may be more helpful.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Ghosh:Ramamoorthi:2002">
        J.&nbsp;K. Ghosh and R.&nbsp;V. Ramamoorthi.
        <BR><SPAN  CLASS="textit">Bayesian Nonparametrics</SPAN>.
        <BR>
Springer, 2002.
        [<A NAME="tex2html5"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1992245">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
The famous book of de Finetti is really a collection of
thoughts, rather than an actual textbook, but is full of interesting
insights and perspectives on a variety of topics.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="deFinetti:1975">
        B.&nbsp;de&nbsp;Finetti.
        <BR><SPAN  CLASS="textit">Theory of Probability, Vol. I &amp; II</SPAN>.
        <BR>
John Wiley &amp; Sons, 1975.
        [<A NAME="tex2html6"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1093666">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
Another useful reference on Bayesian models, exponential families etc
is the following book. (Despite the term ``theory'' in the title, this
text does not involve any mathematical sophistication.)

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Bernardo:Smith:1994">
       J.&nbsp;M. Bernardo and A.&nbsp;F.&nbsp;M. Smith.
       <BR><SPAN  CLASS="textit">Bayesian Theory</SPAN>.
       <BR>
John Wiley &amp; Sons, 1994.
       [<A NAME="tex2html7"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1274699">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<DIV ALIGN="CENTER">

    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">NP Bayesian Models</SPAN>
</font>
        <A NAME="sec:npbmodels"></A>
</DIV>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">Dirichlet processes</SPAN>
</font>
<A NAME="sec:DP"></A>
<P>
The original DP paper is of course Ferguson's 1973 article. In his
acknowledgments, Ferguson attributes the idea to David Blackwell.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Ferguson:1973">
        T.&nbsp;S. Ferguson.
        <BR>
A Bayesian analysis of some nonparametric problems.
        <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 1(2), 1973.
        [<A NAME="tex2html8"
  HREF="http://www.ams.org/mathscinet-getitem?mr=350949">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
At about the same time, Ferguson's student Antoniak introduced a model
called a <SPAN  CLASS="textit">Mixture of Dirichlet Processes</SPAN> (MDP), which is
sometimes mistaken as a Dirichlet Process mixture. The MDP puts a
prior on the parameters of the DP base measure. A draw from a MDP
is discrete almost surely, just as for the DP. However - I am
grateful to Steven MacEachern for pointing this out to me - 
Antoniak also introduces the idea of using a parametric likelihood
with a DP or MDP, which he refers to as ``random noise'' (cf his
Theorem 3) and as a sampling distribution (cf Example 4), so his
paper constitutes the original reference for both the MDP and
for the widely used Dirichlet process mixture model.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Antoniak:1974">
        C.&nbsp;E. Antoniak.
        <BR>
Mixtures of Dirichlet processes with applications to Bayesian
        nonparametric estimation.
        <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 2(6):1152-1174,
        1974.
        [<A NAME="tex2html9"
  HREF="http://www.ams.org/mathscinet-getitem?mr=365969">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
The Dirichlet process mixture, regarded as a smoothed DP obtained by
convolution with a parametric likelihood, was considered in detail by Lo.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Lo:1984">
        A.&nbsp;Y. Lo.
        <BR>
On a class of Bayesian nonparametric estimates. I. Density estimates.
        <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 12(1):351-357,
          1984.
          [<A NAME="tex2html10"
  HREF="http://www.ams.org/mathscinet-getitem?mr=733519">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
The discreteness of DP random draws on arbitrary domains was first shown by David
Blackwell, whom Ferguson's article also credits with originally suggesting
the research that led to the DP's development. The ``P&#243;lya urn''
interpretation is due to James MacQueen.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Blackwell:1973">
        D.&nbsp;Blackwell.
        <BR>
Discreteness of Ferguson selections.
        <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 1(2):356-358, 1973.
        [<A NAME="tex2html11"
  HREF="http://www.ams.org/mathscinet-getitem?mr=348905">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Blackwell:MacQueen:1973">
       D.&nbsp;Blackwell and J.&nbsp;B. MacQueen.
       <BR>
Ferguson distributions via P&#243;lya urn schemes.
       <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 1(2):353-355, 1973.
       [<A NAME="tex2html12"
  HREF="http://www.ams.org/mathscinet-getitem?mr=362614">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">P&#243;lya trees</SPAN>
</font>
<A NAME="sec:PT"></A>
<P>
The original reference is again due to Ferguson. A reasonable reference to get started on P&#243;lya trees, in addition
to the review by Walker et al (see above), are two papers of Lavine.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Ferguson:1974">
        T.&nbsp;S. Ferguson.
        <BR>
Prior distributions on spaces of probability measures.
        <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 2(4):615-629, 1974.
        [<A NAME="tex2html13"
  HREF="http://www.ams.org/mathscinet-getitem?mr=438568">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Lavine:1992">
        M.&nbsp;Lavine.
        <BR>
Some aspects of P&#243;lya tree distributions for statistical
          modelling.
        <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 20(3):1222-1235,
          1992.
         [<A NAME="tex2html14"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1186248">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Lavine:1994">
        M.&nbsp;Lavine.
        <BR>
More aspects of P&#243;lya tree distributions for statistical
                  modelling.
        <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 22(3):1161-1176,
                  1994.
        [<A NAME="tex2html15"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1311970">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">Neutral to the right and L&#233;vy processes</SPAN>
</font>
<A NAME="sec:NTTR"></A>
<P>
NTTR priors were introduced by Kjell Doksum. 
Other references include the Walker et al survey, and the monograph
of Ghosh and Ramamoorthi. The limited direct applicability of L&#233;vy
processes as nonparametric Bayesian priors (ie the fact that a renormalized,
monotonic L&#233;vy process only admits a conjugate posterior if it
is the DP) is shown by James et al.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Doksum:1974">
        K.&nbsp;A. Doksum.
        <BR>
Tailfree and neutral random probabilities and their posterior
          distributions.
          <BR><SPAN  CLASS="textit">Annals of Probability</SPAN>, 2:183-201, 1974.
        [<A NAME="tex2html16"
  HREF="http://www.ams.org/mathscinet-getitem?mr=373081">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="James:Lijoi:Pruenster:2005">
       L.&nbsp;F. James, A.&nbsp;Lijoi, and I.&nbsp;Pr&#252;nster.
       <BR>
Conjugacy as a distinctive feature of the Dirichlet process.
       <BR><SPAN  CLASS="textit">Scand. J. of Statistics</SPAN>, 33:105-120, 2005.
       [<A NAME="tex2html17"
  HREF="http://www.ams.org/mathscinet-getitem?mr=2255112">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">The Pitman-Yor process</SPAN>
</font>
<A NAME="sec:PY"></A>
<P>
A discrete generalization of the DP certainly of interest to
machine learning is the Pitman-Yor process. The most accessible
introduction may be the report below by Yee Whye Teh, 
which applies the model to an illustrative
problem in language processing. The article by Pitman and Yor requires
some courage.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Teh:2006">
        Y.&nbsp;W. Teh.
        <BR>
A Bayesian interpretation of interpolated Kneser-Ney.
        <BR>
Technical report, School of Computing, National University of
          Singapore, 2006.
        [<A NAME="tex2html18"
  HREF="http://www.gatsby.ucl.ac.uk/~ywteh/research/bayesnlp/hpylm.pdf">PDF</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Pitman:Yor:1997">
       J.&nbsp;Pitman and M.&nbsp;Yor.
       <BR>
The two-parameter Poisson-Dirichlet distribution derived from a
         stable subordinator.
       <BR><SPAN  CLASS="textit">Annals of Probability</SPAN>, 25(2):855-900,
         1997.
       [<A NAME="tex2html19"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1434129">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">Consistency and Posterior Convergence</SPAN>
</font>
<A NAME="sec:consistency"></A>
<P>
The most frequently cited paper on the subject is without doubt the article by
Diaconis and Freedman,
which gave an explicit example for an inconsistent DP estimate.
The discussion following the article is worth reading as much as
the article itself.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Diaconis:Freedman:1986">
       P.&nbsp;Diaconis and D.&nbsp;Freedman.
       <BR>
On the consistency of Bayes estimates (with discussion).
       <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 14(1):1-67, 1986.
       [<A NAME="tex2html20"
  HREF="http://www.ams.org/mathscinet-getitem?mr=829555">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
The consistency issue was first raised by David Freedman in the case of random
measures on countable domains (for which he introduced the tailfree
property):

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Freedman:1963">
       D.&nbsp;Freedman.
       <BR>
On the asymptotic behavior of Bayes estimates in the discrete case
         I.
       <BR><SPAN  CLASS="textit">Annals of Mathematical Statistics</SPAN>, 34:1386-1403,
           1963.
       [<A NAME="tex2html21"
  HREF="http://www.ams.org/mathscinet-getitem?mr=158483">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
The proof of ``Bayesian'' consistency (ie consistency for all true
parameter values up to a null set of the prior) for a large
range of models is due to:

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Doob:1949">
        J.&nbsp;L. Doob.
        <BR><SPAN  CLASS="textit">Application of the theory of martingales</SPAN>.
        <BR>
Coll. Int. du CNRS Paris. 1949.
        [<A NAME="tex2html22"
  HREF="http://www.ams.org/mathscinet-getitem?mr=33460">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
The works by Diaconis and Freedman give compelling arguments against
the use of this notion of consistency. References on the modern
theory of NP Bayes consistency and convergence rates, drawing on
empirical process methods (ie concepts familiar from learning theory),
include the following:

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Shen:Wasserman:2001">
       X.&nbsp;Shen and L.&nbsp;Wasserman.
       <BR>
Rates of convergence of posterior distributions.
       <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 29:687-714, 2001.
       [<A NAME="tex2html23"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1865337">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Ghosal:Ghosh:vanDerVaart:2002">
       S.&nbsp;Ghosal, J.&nbsp;K. Ghosh, and A.&nbsp;W. van&nbsp;der Vaart.
       <BR>
Convergence rates of posterior distributions.
       <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 28(2):500-531,
         2002.
       [<A NAME="tex2html24"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1790007">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Kleijn:vanDerVaart:2006">
       B.&nbsp;J.&nbsp;K. Kleijn and A.&nbsp;W. van&nbsp;der Vaart.
       <BR>
Misspecification in infinite-dimensional Bayesian statistics.
       <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 34(2):837-877,
         2006.
       [<A NAME="tex2html25"
  HREF="http://www.ams.org/mathscinet-getitem?mr=2283395">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Ghosal:vanDerWaart:2007">
       S.&nbsp;Ghosal and A.&nbsp;W. van&nbsp;der Vaart.
       <BR>
Posterior convergence rates of Dirichlet mixtures at smooth
         densities.
       <BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 35(2):697-723,
         2007.
       [<A NAME="tex2html26"
  HREF="http://www.ams.org/mathscinet-getitem?mr=2336864">MathSciNet</A>]

<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="vanDerVaart:vanZanten:2008">
A.&nbsp;W. van&nbsp;der Vaart and J.&nbsp;H.&nbsp;van Zanten.
<BR>
Rates of contraction of posterior distributions based on Gaussian
  process priors.
<BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 36(3):1435-1463,
  2008.
[<A NAME="tex2html27"
  HREF="http://www.ams.org/mathscinet-getitem?mr=2418663">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
Concerning the distribution of prior mass on arbitrary
Kullback-Leibler neighborhoods, cf:

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Walker:Damien:Lenk:2004">
S.&nbsp;Walker, P.&nbsp;Damien, and P.&nbsp;Lenk.
<BR>
On priors with a Kullback-Leibler property.
<BR><SPAN  CLASS="textit">Journal of the American Statistical Association</SPAN>, 99:404-408, 2004.
[<A NAME="tex2html28"
  HREF="http://www.ams.org/mathscinet-getitem?mr=2062826">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<DIV ALIGN="CENTER">

    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">Probability Textbooks</SPAN>
</font>
        <A NAME="sec:textbooks"></A>
</DIV>

<P>
My favorite reference are the two volumes by Bauer. In combination
with e.g. Mark Schervish's <EM>Theory of Statistics</EM>, they should
provide all background knowledge required to understand modern 
research papers on Bayesian nonparametrics in detail (with 
perhaps the exception of the
combinatorial approach taken by authors such as Aldous, Pitman, 
Vershik and Yor). 

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Bauer:2001">
H.&nbsp;Bauer.
<BR><SPAN  CLASS="textit">Measure and integration theory</SPAN>.
<BR>
W. de Gruyter, 2001.
[<A NAME="tex2html29"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1897176">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Bauer:1996">
H.&nbsp;Bauer.
<BR><SPAN  CLASS="textit">Probability Theory</SPAN>.
<BR>
W. de Gruyter, 1996.
[<A NAME="tex2html30"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1385460">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
The measure-theoretic formalism of probability theory 
was first introduced in a classic monograph by Kolmogorov.
An English translation is available under the title
<EM>Foundations of the Theory of Probability</EM>
[<A NAME="tex2html47"
  HREF="http://www.ams.org/mathscinet-getitem?mr=79843">MathSciNet</A>].
The customary textbook reference on the subject, however, is the one
by Michel Lo&#232;ve. Its measure theory counterpart
is Paul Halmos' book.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Kolmogorov:1933">
A.&nbsp;N. Kolmogorov.
<BR><SPAN  CLASS="textit">Grundbegriffe der Wahrscheinlichkeitsrechnung</SPAN>.
<BR>
Springer, 1933.
[<A NAME="tex2html31"
  HREF="http://www.ams.org/mathscinet-getitem?mr=494348">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Loeve:1977">
M.&nbsp;Lo&#232;ve.
<BR><SPAN  CLASS="textit">Probability Theory, Vol. I &amp; II</SPAN>.
<BR>
Springer, 1977.
[<A NAME="tex2html32"
  HREF="http://www.ams.org/mathscinet-getitem?mr=651018">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Halmos:1974">
P.&nbsp;Halmos.
<BR><SPAN  CLASS="textit">Measure Theory</SPAN>.
<BR>
Springer, 1974.
[<A NAME="tex2html33"
  HREF="http://www.ams.org/mathscinet-getitem?mr=33869">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<DIV ALIGN="CENTER">

    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">Mathematical topics</SPAN>
</font>
        <A NAME="sec:math"></A>
</DIV>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">Exchangeability</SPAN>
</font>
<A NAME="sec:exch"></A>
<P>
The most comprehensive and rigorous treatise on exchangeability I am
aware of is:

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Kallenberg:2005">
O.&nbsp;Kallenberg.
<BR><SPAN  CLASS="textit">Probabilistic Symmetries and Invariance Principles</SPAN>.
<BR>
Springer, 2005.
[<A NAME="tex2html34"
  HREF="http://www.ams.org/mathscinet-getitem?mr=2161313">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
Work on the equivalence of exchangeability and conditional
independence dates back to several publications of de Finetti
on exchangeability of binary random variables
in the early 1930s, such as:

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="deFinetti:1931">
B.&nbsp;de&nbsp;Finetti.
<BR>
Fuzione caratteristica di un fenomeno aleatorio.
<BR><SPAN  CLASS="textit">Atti della R. Academia Nazionale dei Lincei</SPAN>, 4:251-299, 1931.
<P></A>
<DD> </SMALL>

<P>
For de Finetti's perspective on the subject, see his <EM>Theory of
Probability</EM> referenced above. The generalization to arbitrary
random variables, as well as the interpretation of the set of
exchangeable measures as a convex polytope, is due to:

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Hewitt:Savage:1955">
E.&nbsp;Hewitt and L.&nbsp;J. Savage.
<BR>
Symmetric measures on Cartesian products.
<BR><SPAN  CLASS="textit">Transactions of the American Mathematical Society</SPAN>, 80(2):470-501,
1955.
[<A NAME="tex2html35"
  HREF="http://www.ams.org/mathscinet-getitem?mr=76206">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
B&#252;hlmann established exchangeable analogues of the well-known
i.i.d. limit theorems. For anybody interested in exchangeability
(and familiar with either French or German), these make a great read.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Buehlmann:1958">
H.&nbsp;B&#252;hlmann.
<BR>
Le probl&#232;me limite central pour les variables al&#232;atoires
  &#232;changeables.
<BR><SPAN  CLASS="textit">Comptes Rendus Ac. Sc. Paris</SPAN>, 246:534-536, 1958.
[<A NAME="tex2html36"
  HREF="http://www.ams.org/mathscinet-getitem?mr=92265">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Buehlmann:1960">
H.&nbsp;B&#252;hlmann.
<BR><SPAN  CLASS="textit">Austauschbare stochastische Variabeln und ihre
  Grenzwerts&#228;tze</SPAN>.
<BR>
PhD thesis, 1960.
<BR>
University of California Press, 1960.
[<A NAME="tex2html37"
  HREF="http://www.ams.org/mathscinet-getitem?mr=117779">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">Sufficiency</SPAN>
</font>
<A NAME="sec:sufficiency"></A>
<P>
A comprehensive discussion of sufficiency is given in Chapter 2 of
Schervish's book. The concept of a sufficient statistic was introduced
by Fisher. The idea was reformulated in terms of measure theory by Halmos and
Savage, who introduced the notion of a sufficient sigma-algebra.
The consequences of the undominated case for sufficiency were first
considered in detail by Bahadur.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Fisher:1922">
R.&nbsp;A. Fisher.
<BR>
On the mathematical foundations of theoretical statistics.
<BR><SPAN  CLASS="textit">Phil. Trans. Roy. Soc. A</SPAN>, 222:309-368, 1922.
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Halmos:Savage:1949">
P.&nbsp;R. Halmos and L.&nbsp;J. Savage.
<BR>
Application of the Radon-Nikodym theorem to the theory of
  sufficient statistics.
<BR><SPAN  CLASS="textit">Ann. Math. Stat.</SPAN>, 20:225-241, 1949.
[<A NAME="tex2html38"
  HREF="http://www.ams.org/mathscinet-getitem?mr=30730">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Bahadur:1954">
R.&nbsp;R. Bahadur.
<BR>
Sufficiency and statistical decision functions.
<BR><SPAN  CLASS="textit">Ann. Math. Statist.</SPAN>, 25:423-462, 1954.
[<A NAME="tex2html39"
  HREF="http://www.ams.org/mathscinet-getitem?mr=63630">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
In the dominated case, the classical notion of sufficiency (Fisher's
definition) and the Bayesian definition (complete determination of the
posterior by the value of the sufficient statistic) are equivalent.
Blackwell and Ramamoorthi show that this need not be the case for
undominated families of probability measures.

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Blackwell:Ramamoorthi:1982">
D.&nbsp;Blackwell and R.&nbsp;V. Ramamoorthi.
<BR>
A Bayes but not classically sufficient statistic.
<BR><SPAN  CLASS="textit">Annals of Statistics</SPAN>, 10(3):1025-1026,
  1982.
[<A NAME="tex2html40"
  HREF="http://www.ams.org/mathscinet-getitem?mr=663456">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>

  
    <font align=right face=arial,helvetica size=1 color="black"
    STYLE="font-family:arial,helvetica;font-size:14pt;">
<SPAN  CLASS="textbf">Pitman-Koopman Theory</SPAN>
</font>
<A NAME="sec:pitman:koopman"></A>
<P>
The term <EM>Pitman-Koopman theory</EM> is used to refer to a collection of results which
essentially state that a probability model admits a sufficient
statistic of fixed dimension (ie of dimension bounded with respect to
sample size) if and only if it is an exponential family model.
The first proof of such a result is due to Darmois, who published
his result in French. Two English versions independently appeared
a year later, by Pitman and Koopman. Dozens of results have 
since been published, which all differ from one another in terms
of their technical assumptions. The introduction of the paper
by Barankin and Maitra provides an overview (though a substantial
number of results appeared only afterwards).

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Darmois:1935">
G.&nbsp;Darmois.
<BR>
Sur les lois de probabilite a estimation exhaustive.
<BR><SPAN  CLASS="textit">C. R. Acad. Sci. Paris</SPAN>, 260:1265-1266,
1935.
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Koopman:1936">
B.&nbsp;Koopman.
<BR>
On distributions admitting a sufficient statistic.
<BR><SPAN  CLASS="textit">Transactions of the American Mathematical Society</SPAN>,
  39:399-409, 1936.
[<A NAME="tex2html41"
  HREF="http://www.ams.org/mathscinet-getitem?mr=1501854">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Pitman:1936">
E.&nbsp;J.&nbsp;G. Pitman.
<BR>
Sufficient statistics and intrinsic accuracy.
<BR><SPAN  CLASS="textit">Proceedings of the Cambridge Philosophical Society</SPAN>,
  32:567-579, 1936.
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Jeffreys:1961">
H.&nbsp;Jeffreys.
<BR><SPAN  CLASS="textit">Theory of probability</SPAN>.
<BR>
Oxford University Press, 1961.
[<A NAME="tex2html42"
  HREF="http://www.ams.org/mathscinet-getitem?mr=187257">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>
<SMALL CLASS="FOOTNOTESIZE">
<DT><A NAME="Barankin:Maitra:1963">
E.&nbsp;M. Barankin and A.&nbsp;P. Maitra.
<BR>
Generalization of the Fisher-Darmois-Koopman-Pitman theorem
  on sufficient statistics.
<BR><SPAN  CLASS="textit">Sankhya</SPAN>, 25:217-244, 1963.
[<A NAME="tex2html43"
  HREF="http://www.ams.org/mathscinet-getitem?mr=171342">MathSciNet</A>]
<P></A>
<DD> </SMALL>

<P>

<P>
<BR><HR>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-10755739-1");
pageTracker._trackPageview();
} catch(err) {}</script>
</BODY>
</HTML>
