<html>
<head>
 <link rel="alternate" type="application/rss+xml" href="http://www.jmlr.org/jmlr.xml" title="JMLR RSS">
<link rel="stylesheet" type="text/css" href="http://www.jmlr.org/style.css">
<style>. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>
<style type="text/css">
<!-- 
#fixed {
    position: absolute;
    top: 0;
    left: 0;
    width: 8em;
    height: 100%;
}
body > #fixed {
    position: fixed;
}
#content {
    margin-top: 1em;
    margin-left: 10em;
    margin-right: 0.5em;
}
img.jmlr {
    width: 7em;
}
img.rss {
    width: 2em;
}
-->
</style>
<script LANGUAGE='JavaScript'> 
<!-- function GoAddress(user,machine) {
document.location = 'mailto:' + user + '@' + machine; } 
// -->
</SCRIPT>



<style>
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>
</head>
<body>
 <div id="content">

<h2> A Survey of Accuracy Evaluation Metrics of Recommendation Tasks </h2>
<p><b><i>Asela Gunawardana, Guy Shani</i></b>; 10(Dec):2935&minus;2962, 2009.</p>

<h3>Abstract</h3>

<i>Recommender systems</i> are now popular both commercially
and in the research community, where many algorithms have been
suggested for providing recommendations. These algorithms typically
perform differently in various domains and tasks. Therefore, it is
important from the research perspective, as well as from a practical
view, to be able to decide on an algorithm that matches the domain
and the task of interest. The standard way to make such decisions is
by comparing a number of algorithms offline using some evaluation
metric. Indeed, many evaluation metrics have been suggested for
comparing recommendation algorithms. The decision on the proper
evaluation metric is often critical, as each metric may favor a
different algorithm. In this paper we review the proper
construction of offline experiments for deciding on the most
appropriate algorithm. We discuss three important tasks of
recommender systems, and classify a set of appropriate well known
evaluation metrics for each task. We demonstrate how using an
improper evaluation metric can lead to the selection of an improper
algorithm for the task of interest. We also discuss other important
considerations when designing offline experiments.

<font color="gray"><p>[abs]</font>[<a target="_blank" href="http://www.jmlr.org/papers/volume10/gunawardana09a/gunawardana09a.pdf">pdf</a>]

</div>
 <div id="fixed">
<br>
<a align="right" href="http://www.jmlr.org" target=_top><img align="right" class="jmlr" src="/jmlr.jpg" border="0"></a> 
<p><br><br>
<p align="right"> <A href="http://www.jmlr.org/"> Home Page </A> 

<p align="right"> <A href="/papers"> Papers </A> 

<p align="right"> <A href="/author-info.html"> Submissions </A> 

<p align="right"> <A href="/news.html"> News </A> 

<p align="right"> <A href="/scope.html"> Scope </A>

<p align="right"> <A href="/editorial-board.html"> Editorial Board </A> 

<p align="right"> <A href="/announcements.html"> Announcements </A>

<p align="right"> <A href="/proceedings"> Proceedings </A>

<p align="right"> <A href="/mloss">Open Source Software</A>

<p align="right"> <A href="/search-jmlr.html"> Search </A>

<p align="right"> <A href="/manudb"> Login </A></p>

<br><br>
<p align="right"> <A href="/jmlr.xml"> 
<img src="/RSS.gif" class="rss" alt="RSS Feed">
</A>



</div>
 
  </body>

</html>
